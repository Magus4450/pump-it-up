{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.preprocessing import get_processed_data\n",
    "from src.train import train_and_evaluate_baseline, evaluate, load_file_if_exists, save_file, plot_f1_scores, train_w_hpam_search_and_eval\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    ")\n",
    "sns.set_theme('paper')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"status_group\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325bd9d",
   "metadata": {},
   "source": [
    "## BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db040690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"baseline_log_reg_all_feat\"\n",
    "train_f1, test_f1, model = train_and_evaluate_baseline(\n",
    "    preset_name=\"\", \n",
    "    model_name=model_name,\n",
    "    seed=SEED, \n",
    "    model_kwargs={\n",
    "        \"max_iter\": 1000\n",
    "    }\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e22ab2",
   "metadata": {},
   "source": [
    "## BASELINE + feature_engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"baseline_log_reg_all_feat_fe\"\n",
    "train_f1, test_f1, model = train_and_evaluate_baseline(\n",
    "    preset_name=\"feature_engineer\", \n",
    "    model_name=model_name,\n",
    "    seed=SEED, \n",
    "    model_kwargs={\n",
    "        \"max_iter\": 1000\n",
    "    }\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43883eef",
   "metadata": {},
   "source": [
    "## BASELINE + Log_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4609c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"baseline_log_reg_all_feat_log_transform\"\n",
    "train_f1, test_f1, model = train_and_evaluate_baseline(\n",
    "    preset_name=\"log_transform\", \n",
    "    model_name=model_name,\n",
    "    seed=SEED, \n",
    "    model_kwargs={\n",
    "        \"max_iter\": 1000\n",
    "    }\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3044a54",
   "metadata": {},
   "source": [
    "## BASELINE + log_transform + feature_engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"baseline_log_reg_all_feat_log_transform_fe\"\n",
    "train_f1, test_f1, model = train_and_evaluate_baseline(\n",
    "    preset_name=\"log_transform+feature_engineer \", \n",
    "    model_name=model_name,\n",
    "    seed=SEED, \n",
    "    model_kwargs={\n",
    "        \"max_iter\": 1000\n",
    "    }\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c8896",
   "metadata": {},
   "source": [
    "## BASELINE + log_transform + feature_engineer + remove_correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e874ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"baseline_log_reg_non_cor_feat_log_transform_fe\"\n",
    "train_f1, test_f1, model = train_and_evaluate_baseline(\n",
    "    preset_name=\"log_transform+feature_engineer+remove_correlated\", \n",
    "    model_name=model_name,\n",
    "    seed=SEED, \n",
    "    model_kwargs={\n",
    "        \"max_iter\": 1000\n",
    "    }\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e67a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_scores(result_dict):\n",
    "    # Convert dict â†’ DataFrame\n",
    "    df = pd.DataFrame(result_dict).T.reset_index()\n",
    "    df.columns = [\"Model\", \"Train F1\", \"Test F1\"]\n",
    "\n",
    "    # Melt for seaborn\n",
    "    df_melted = df.melt(id_vars=\"Model\", value_vars=[\"Train F1\", \"Test F1\"],\n",
    "                        var_name=\"Dataset\", value_name=\"F1 Score\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df_melted, x=\"Model\", y=\"F1 Score\", hue=\"Dataset\")\n",
    "\n",
    "    plt.title(\"Macro F1 Scores by Model\")\n",
    "    plt.ylim(0, 1)  # since F1 is bounded\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.legend(title=\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd61638",
   "metadata": {},
   "source": [
    "## Complex Models with Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix preset\n",
    "PRESET_w_all = \"log_transform+feature_engineer\"\n",
    "PRESET_wo_all = \"log_transform+feature_engineer+remove_correlated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6bb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"random_forest_hpam_tuned_all\"\n",
    "rf = RandomForestClassifier(class_weight=\"balanced_subsample\")\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(20, 300),        # number of trees\n",
    "    \"max_depth\": randint(3, 20),             # depth of trees\n",
    "    \"min_samples_split\": randint(2, 20),     # min samples to split\n",
    "    \"min_samples_leaf\": randint(1, 20),      # min samples per leaf\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]   # features per split\n",
    "}\n",
    "\n",
    "train_f1, test_f1, model = train_w_hpam_search_and_eval(\n",
    "    model=rf,\n",
    "    preset_name=PRESET_w_all,\n",
    "    model_name=model_name,\n",
    "    param_dist=param_dist,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf8f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"random_forest_hpam_tuned_no_all\"\n",
    "rf = RandomForestClassifier(class_weight=\"balanced_subsample\")\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(20, 300),        # number of trees\n",
    "    \"max_depth\": randint(3, 20),             # depth of trees\n",
    "    \"min_samples_split\": randint(2, 20),     # min samples to split\n",
    "    \"min_samples_leaf\": randint(1, 20),      # min samples per leaf\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]   # features per split\n",
    "}\n",
    "\n",
    "train_f1, test_f1, model = train_w_hpam_search_and_eval(\n",
    "    model=rf,\n",
    "    preset_name=PRESET_wo_all,\n",
    "    model_name=model_name,\n",
    "    param_dist=param_dist,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"random_forest_hpam_tuned_all\"\n",
    "rf = RandomForestClassifier(class_weight=\"balanced_subsample\")\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(50, 300),        # number of trees\n",
    "    \"max_depth\": randint(3, 20),             # depth of trees\n",
    "    \"min_samples_split\": randint(2, 20),     # min samples to split\n",
    "    \"min_samples_leaf\": randint(1, 20),      # min samples per leaf\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]   # features per split\n",
    "}\n",
    "\n",
    "train_f1, test_f1, model = train_w_hpam_search_and_eval(\n",
    "    model=rf,\n",
    "    preset_name=PRESET_w_all,\n",
    "    model_name=model_name,\n",
    "    param_dist=param_dist,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5736be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"random_forest_hpam_tuned_no_all\"\n",
    "rf = RandomForestClassifier(class_weight=\"balanced_subsample\")\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(50, 300),        # number of trees\n",
    "    \"max_depth\": randint(3, 20),             # depth of trees\n",
    "    \"min_samples_split\": randint(2, 20),     # min samples to split\n",
    "    \"min_samples_leaf\": randint(1, 20),      # min samples per leaf\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]   # features per split\n",
    "}\n",
    "\n",
    "train_f1, test_f1, model = train_w_hpam_search_and_eval(\n",
    "    model=rf,\n",
    "    preset_name=PRESET_wo_all,\n",
    "    model_name=model_name,\n",
    "    param_dist=param_dist,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef03ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"adaboost_hpam_tuned_all\"\n",
    "adb = AdaBoostClassifier(random_state=SEED)\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(50, 1000),\n",
    "    \"learning_rate\": loguniform(1e-3, 1.0),\n",
    "    \"algorithm\": [\"SAMME\", \"SAMME.R\"],\n",
    "}\n",
    "train_f1, test_f1, model = train_w_hpam_search_and_eval(\n",
    "    model=adb, preset_name=PRESET_w_all, model_name=model_name, param_dist=param_dist, seed=SEED\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"adaboost_hpam_tuned_no_all\"\n",
    "adb = AdaBoostClassifier(random_state=SEED)\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(50, 1000),\n",
    "    \"learning_rate\": loguniform(1e-3, 1.0),\n",
    "    \"algorithm\": [\"SAMME\", \"SAMME.R\"],\n",
    "}\n",
    "train_f1, test_f1, model = train_w_hpam_search_and_eval(\n",
    "    model=adb, preset_name=PRESET_wo_all, model_name=model_name, param_dist=param_dist, seed=SEED\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hgb_hpam_tuned_all\"\n",
    "hgb = HistGradientBoostingClassifier(random_state=SEED)\n",
    "param_dist = {\n",
    "    \"learning_rate\": loguniform(1e-3, 1.0),\n",
    "    \"max_depth\": randint(2, 20),\n",
    "    \"max_leaf_nodes\": randint(15, 255),\n",
    "    \"min_samples_leaf\": randint(10, 200),\n",
    "    \"l2_regularization\": loguniform(1e-2, 1e2),\n",
    "    \"max_bins\": randint(64, 255),\n",
    "}\n",
    "train_f1, test_f1, model = train_w_hpam_search_and_eval(\n",
    "    model=hgb, preset_name=PRESET_w_all, model_name=model_name, param_dist=param_dist, seed=SEED\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hgb_hpam_tuned_no_all\"\n",
    "hgb = HistGradientBoostingClassifier(random_state=SEED)\n",
    "param_dist = {\n",
    "    \"learning_rate\": loguniform(1e-3, 1.0),\n",
    "    \"max_depth\": randint(2, 20),\n",
    "    \"max_leaf_nodes\": randint(15, 255),\n",
    "    \"min_samples_leaf\": randint(10, 200),\n",
    "    \"l2_regularization\": loguniform(1e-2, 1e2),\n",
    "    \"max_bins\": randint(64, 255),\n",
    "}\n",
    "train_f1, test_f1, model = train_w_hpam_search_and_eval(\n",
    "    model=hgb, preset_name=PRESET_wo_all, model_name=model_name, param_dist=param_dist, seed=SEED\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed650f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"xgb_hpam_tuned_all\"\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softprob\",   # multiclass\n",
    "    num_class=3,  # ensure correct #classes\n",
    "    tree_method=\"hist\",           # fast, CPU-friendly; use \"gpu_hist\" if you have GPU\n",
    "    random_state=SEED,\n",
    "    eval_metric=\"mlogloss\"        # metric for training; CV scoring will be f1_macro\n",
    ")\n",
    "\n",
    "# --- hyperparameter search space ---\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(50, 500),\n",
    "    \"max_depth\": randint(3, 10),\n",
    "    \"min_child_weight\": loguniform(1e-1, 1e2),\n",
    "    \"subsample\": uniform(0.5, 0.5),         # 0.5â€“1.0\n",
    "    \"colsample_bytree\": uniform(0.5, 0.5),  # 0.5â€“1.0\n",
    "    \"gamma\": loguniform(1e-4, 1.0),\n",
    "    \"learning_rate\": loguniform(1e-3, 3e-1),\n",
    "    \"reg_alpha\": loguniform(1e-3, 1e0),     # L1\n",
    "    \"reg_lambda\": loguniform(1e-3, 1e2),    # L2\n",
    "}\n",
    "\n",
    "# ----- OPTION A: if your helper supports fit_params -----\n",
    "# (Recommended: add a `fit_params` dict argument to your helper and pass it to RandomizedSearchCV.fit)\n",
    "train_f1, test_f1, model = train_w_hpam_search_and_eval(\n",
    "    model=xgb,\n",
    "    preset_name=PRESET_w_all,\n",
    "    model_name=model_name,\n",
    "    param_dist=param_dist,\n",
    "    seed=SEED,\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"xgb_hpam_tuned_no_all\"\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softprob\",   # multiclass\n",
    "    num_class=3,  # ensure correct #classes\n",
    "    tree_method=\"hist\",           # fast, CPU-friendly; use \"gpu_hist\" if you have GPU\n",
    "    random_state=SEED,\n",
    "    eval_metric=\"mlogloss\"        # metric for training; CV scoring will be f1_macro\n",
    ")\n",
    "\n",
    "# --- hyperparameter search space ---\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(50, 500),\n",
    "    \"max_depth\": randint(3, 10),\n",
    "    \"min_child_weight\": loguniform(1e-1, 1e2),\n",
    "    \"subsample\": uniform(0.5, 0.5),         # 0.5â€“1.0\n",
    "    \"colsample_bytree\": uniform(0.5, 0.5),  # 0.5â€“1.0\n",
    "    \"gamma\": loguniform(1e-4, 1.0),\n",
    "    \"learning_rate\": loguniform(1e-3, 3e-1),\n",
    "    \"reg_alpha\": loguniform(1e-3, 1e0),     # L1\n",
    "    \"reg_lambda\": loguniform(1e-3, 1e2),    # L2\n",
    "}\n",
    "\n",
    "# ----- OPTION A: if your helper supports fit_params -----\n",
    "# (Recommended: add a `fit_params` dict argument to your helper and pass it to RandomizedSearchCV.fit)\n",
    "train_f1, test_f1, model = train_w_hpam_search_and_eval(\n",
    "    model=xgb,\n",
    "    preset_name=PRESET_wo_all,\n",
    "    model_name=model_name,\n",
    "    param_dist=param_dist,\n",
    "    seed=SEED,\n",
    ")\n",
    "print(f\"{model_name}\\nTrain: {train_f1:.4f}\\nTest: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pump-it-up (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
