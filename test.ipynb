{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0d868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading and Preprocessing Data ---\n",
      "{'all_columns': ['id', 'amount_tsh', 'date_recorded', 'funder', 'gps_height', 'installer', 'longitude', 'latitude', 'wpt_name', 'num_private', 'basin', 'subvillage', 'region', 'region_code', 'district_code', 'lga', 'ward', 'population', 'public_meeting', 'recorded_by', 'scheme_management', 'scheme_name', 'permit', 'construction_year', 'extraction_type', 'extraction_type_group', 'extraction_type_class', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'quantity_group', 'source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group', 'status_group'], 'remove_col_after_log': True, 'cat_col_cut_off': 10, 'cat_columns': ['installer', 'wpt_name', 'basin', 'public_meeting', 'scheme_management', 'permit', 'extraction_type', 'management', 'payment', 'water_quality', 'quantity', 'quantity_group', 'waterpoint_type', 'recorded_by'], 'log_transform_cols': ['amount_tsh', 'population'], 'feature_engineer': True}\n",
      "Data preprocessing complete.\n",
      "\n",
      "--- Starting Cross-Validated Hyperparameter Search ---\n",
      "\n",
      "Searching for best parameters for: logistic_regression...\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "\n",
      "Searching for best parameters for: decision_tree...\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "\n",
      "Searching for best parameters for: random_forest...\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "\n",
      "Searching for best parameters for: gradient_boosting...\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "\n",
      "Searching for best parameters for: xgboost...\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "\n",
      "--- Hyperparameter Search Complete ---\n",
      "\n",
      "--- Hyperparameter Search Summary ---\n",
      "                                                           best_params  \\\n",
      "random_forest        {'classifier__max_depth': 40, 'classifier__max...   \n",
      "xgboost              {'classifier__colsample_bytree': 0.9, 'classif...   \n",
      "decision_tree        {'classifier__max_depth': 40, 'classifier__max...   \n",
      "logistic_regression              {'classifier__C': 0.0745934328572655}   \n",
      "gradient_boosting    {'classifier__learning_rate': 0.00846800857524...   \n",
      "\n",
      "                    best_f1_macro best_balanced_acc  \n",
      "random_forest            0.638283          0.697166  \n",
      "xgboost                  0.634014          0.609271  \n",
      "decision_tree            0.555196          0.635549  \n",
      "logistic_regression      0.548629          0.616409  \n",
      "gradient_boosting        0.531847          0.524271  \n",
      "\n",
      "--- Winning Model: random_forest ---\n",
      "Best Parameters Found: {'classifier__max_depth': 40, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 3, 'classifier__n_estimators': 271}\n",
      "\n",
      "--- Final Model Evaluation on Full Training Data ---\n",
      "\n",
      "Confusion Matrix (Full Train Set):\n",
      "[[24693  5149  2417]\n",
      " [  533  3558   226]\n",
      " [ 3603  2122 17099]]\n",
      "\n",
      "Classification Report (Full Train Set):\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional      0.857     0.765     0.808     32259\n",
      "functional needs repair      0.329     0.824     0.470      4317\n",
      "         non functional      0.866     0.749     0.803     22824\n",
      "\n",
      "               accuracy                          0.763     59400\n",
      "              macro avg      0.684     0.780     0.694     59400\n",
      "           weighted avg      0.822     0.763     0.782     59400\n",
      "\n",
      "\n",
      "--- Generating Submission File: submission_random_forest.csv ---\n",
      "Submission file saved successfully!\n",
      "\n",
      "--- Project Finished ---\n",
      "Final submission head:\n",
      "      id    status_group\n",
      "0  50785  non functional\n",
      "1  51630      functional\n",
      "2  17168  non functional\n",
      "3  45559  non functional\n",
      "4  49871      functional\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import our custom modules\n",
    "from src.preprocessing.presets import get_preset \n",
    "from src.model.models import create_and_return_all_models\n",
    "from src.hyperparameter_tuning import get_param_dists, run_hyperparameter_search\n",
    "from src.train.evaluate import show_final_evaluation, create_submission_file\n",
    "\n",
    "\n",
    "# ## 2. Load and Preprocess Data\n",
    "print(\"--- Step 1: Loading and Preprocessing Data ---\")\n",
    "train_values = pd.read_csv(\"data/train_set_values.csv\")\n",
    "train_labels = pd.read_csv(\"data/train_set_labels.csv\")\n",
    "test_values  = pd.read_csv(\"data/test_set_values.csv\")\n",
    "train_df = pd.merge(train_values, train_labels, on=\"id\", how=\"left\")\n",
    "\n",
    "# Use your predefined preprocessing pipeline from presets.py\n",
    "preset_name = \"log_transform+remove_correlated+feature_engineer\"\n",
    "pre = get_preset(preset_name, list(train_df.columns))\n",
    "\n",
    "train_processed = pre.fit_transform(train_df)\n",
    "test_processed  = pre.transform(test_values)\n",
    "print(\"Data preprocessing complete.\")\n",
    "\n",
    "\n",
    "# ## 3. Prepare Data for Modeling\n",
    "# Separate features (X) and target (y), and encode the target variable\n",
    "X = train_processed.drop(columns=[\"status_group\"])\n",
    "y_raw = train_processed[\"status_group\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "\n",
    "# ## 4. Run Cross-Validated Hyperparameter Search\n",
    "# Get the dictionary of untrained model pipelines from models.py\n",
    "# We pass X so the function can identify numeric columns for the scaler\n",
    "models_to_tune = create_and_return_all_models(X_train=X, seed=42)\n",
    "\n",
    "# Get the dictionary of parameter distributions to search over\n",
    "param_dists = get_param_dists()\n",
    "\n",
    "# Run the search. Set n_iter to a higher number (e.g., 25) for a real search.\n",
    "summary_df, best_estimators = run_hyperparameter_search(models_to_tune, param_dists, X, y, n_iter=1)\n",
    "\n",
    "print(\"\\n--- Hyperparameter Search Summary ---\")\n",
    "print(summary_df)\n",
    "\n",
    "\n",
    "# ## 5. Evaluate and Submit the Winning Model\n",
    "# Select the best model based on the f1_macro score from the summary table\n",
    "winner_name = summary_df.index[0]\n",
    "winner_model = best_estimators[winner_name]\n",
    "\n",
    "print(f\"\\n--- Winning Model: {winner_name} ---\")\n",
    "print(f\"Best Parameters Found: {summary_df.loc[winner_name, 'best_params']}\")\n",
    "\n",
    "# **This is the new step that shows the detailed report you wanted.**\n",
    "# The model from RandomizedSearchCV is already refit on the full data.\n",
    "# We call our new function from evaluation.py to see the results.\n",
    "show_final_evaluation(winner_model, X, y, le)\n",
    "\n",
    "# Create the final submission file using the fully trained winner\n",
    "X_test = test_processed.copy()\n",
    "submission_df = create_submission_file(\n",
    "    model=winner_model, \n",
    "    X_test=X_test, \n",
    "    original_test_df=test_values, \n",
    "    label_encoder=le,\n",
    "    filename=f\"submission_{winner_name.replace(' ', '_')}.csv\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Project Finished ---\")\n",
    "print(\"Final submission head:\")\n",
    "print(submission_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pump-it-up (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
