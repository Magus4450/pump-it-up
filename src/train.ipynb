{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from preprocessing import get_preset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = pd.read_csv(\"../data/train_set_values.csv\")\n",
    "train_labels = pd.read_csv(\"../data/train_set_labels.csv\")\n",
    "test_values  = pd.read_csv(\"../data/test_set_values.csv\")\n",
    "\n",
    "# Merge labels into train only\n",
    "train_df = pd.merge(train_values, train_labels, on=\"id\", how=\"left\")\n",
    "\n",
    "preset_name = \"log_transform+remove_correlated+feature_engineer\"\n",
    "pre = get_preset(preset_name, list(train_df.columns))\n",
    "\n",
    "train_processed = pre.fit_transform(train_df)\n",
    "test_processed  = pre.transform(test_values)\n",
    "\n",
    "print(\"Train shape:\", train_processed.shape)\n",
    "print(\"Test  shape:\", test_processed.shape)\n",
    "\n",
    "print(sum(train_processed.isna().sum()))\n",
    "print(sum(test_processed.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a555af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sTarget = \"status_group\" == col. for col in train_processed.columns \n",
    "train_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b45e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: assuming train_processed and test_processed are pandas DataFrames\n",
    "# Split features (X) and target (y) from train_processed\n",
    "X = train_processed.drop([\"status_group\"], axis=1)\n",
    "y = train_processed[\"status_group\"]\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "            n_estimators=400, max_depth=None, class_weight=\"balanced_subsample\",\n",
    "            n_jobs=-1, random_state=SEED\n",
    "        )\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# scores = cross_val_score(model, X, y, cv=kf, scoring=\"r2\") \n",
    "\n",
    "# print(\"Cross-validation scores:\", scores)\n",
    "# print(\"Mean CV score:\", np.mean(scores))\n",
    "\n",
    "# Fit the model on the full training set after CV\n",
    "model.fit(X, y)\n",
    "\n",
    "# Generate predictions on test set\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "f1_scor = f1_score(y, y_pred, average=\"weighted\")\n",
    "print(f1_scor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_processed.drop(columns=[\"status_group\"])\n",
    "y = train_processed[\"status_group\"]\n",
    "X_test = test_processed.copy()\n",
    "\n",
    "# Optional: if you have mixed dtypes, define columns and scale only numerics:\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[(\"scale\", StandardScaler(with_mean=True), num_cols)],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "pipelines = {\n",
    "    # \"logreg_multinomial\": Pipeline([\n",
    "    #     (\"prep\", preprocess),\n",
    "    #     (\"clf\", LogisticRegression(\n",
    "    #         multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=2000,\n",
    "    #         class_weight=\"balanced\"\n",
    "    #     ))\n",
    "    # ]),\n",
    "    # \"linear_svm_calibrated\": Pipeline([\n",
    "    #     (\"prep\", preprocess),\n",
    "    #     (\"clf\", CalibratedClassifierCV(\n",
    "    #         estimator=LinearSVC(class_weight=\"balanced\"),\n",
    "    #         method=\"sigmoid\", cv=3\n",
    "    #     ))\n",
    "    # ]),\n",
    "    \"random_forest\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            class_weight=\"balanced_subsample\", n_jobs=-1, random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    # \"hist_gbdt\": Pipeline([\n",
    "    #     (\"prep\", preprocess),\n",
    "    #     (\"clf\", HistGradientBoostingClassifier(random_state=42))\n",
    "    # ]),\n",
    "    # \"knn\": Pipeline([\n",
    "    #     (\"prep\", preprocess),\n",
    "    #     (\"clf\", KNeighborsClassifier())\n",
    "    # ]),\n",
    "}\n",
    "\n",
    "# --- 3) Parameter distributions (note the 'clf__' prefix)\n",
    "param_dists = {\n",
    "    # \"logreg_multinomial\": {\n",
    "    #     \"clf__C\": loguniform(1e-3, 1e2),   # ~[0.001, 100]\n",
    "    # },\n",
    "    # \"linear_svm_calibrated\": {\n",
    "    #     \"clf__estimator__C\": loguniform(1e-3, 1e2),\n",
    "    #     \"clf__method\": [\"sigmoid\", \"isotonic\"],  # categorical choices are fine\n",
    "    # },\n",
    "    \"random_forest\": {\n",
    "        \"clf__n_estimators\": randint(150, 1001),\n",
    "        \"clf__max_depth\": [None, 10, 20, 40, 80],\n",
    "        \"clf__min_samples_leaf\": randint(1, 6),\n",
    "        \"clf__max_features\": [\"sqrt\", None],\n",
    "    },\n",
    "    # \"hist_gbdt\": {\n",
    "    #     \"clf__learning_rate\": loguniform(1e-3, 3e-1),  # ~[0.001, 0.3]\n",
    "    #     \"clf__max_depth\": [None, 3, 6, 9],\n",
    "    #     \"clf__max_leaf_nodes\": randint(15, 256),\n",
    "    #     \"clf__max_iter\": randint(150, 601),\n",
    "    #     \"clf__l2_regularization\": loguniform(1e-8, 10.0),\n",
    "    # },\n",
    "    # \"knn\": {\n",
    "    #     \"clf__n_neighbors\": randint(3, 64),\n",
    "    #     \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "    #     \"clf__p\": [1, 2],  # Manhattan vs Euclidean\n",
    "    # },\n",
    "}\n",
    "\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=SEED)\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"f1_macro\": \"f1_macro\",\n",
    "    \"balanced_acc\": \"balanced_accuracy\",\n",
    "    \"roc_auc_ovr\": \"roc_auc_ovr_weighted\"\n",
    "}\n",
    "\n",
    "N_ITER = 1\n",
    "search_results = {}\n",
    "best_estimators = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pipe in pipelines.items():\n",
    "    dist = param_dists[name]\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=dist,\n",
    "        n_iter=N_ITER,\n",
    "        scoring=scoring,\n",
    "        refit=\"f1_macro\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    best_estimators[name] = search.best_estimator_\n",
    "    search_results[name] = {\n",
    "        \"best_params\": search.best_params_,\n",
    "        \"best_f1_macro\": search.best_score_,\n",
    "        \"best_accuracy\": search.cv_results_[\"mean_test_accuracy\"][search.best_index_],\n",
    "        \"best_balanced_acc\": search.cv_results_[\"mean_test_balanced_acc\"][search.best_index_],\n",
    "        \"best_roc_auc_ovr\": search.cv_results_[\"mean_test_roc_auc_ovr\"][search.best_index_],\n",
    "    }\n",
    "\n",
    "# Compare models\n",
    "summary_df = pd.DataFrame(search_results).T.sort_values(\"best_f1_macro\", ascending=False)\n",
    "print(summary_df)\n",
    "\n",
    "# --- 5) Pick winner, fit on full data, diagnostics\n",
    "winner_name = summary_df.index[0]\n",
    "winner = best_estimators[winner_name]\n",
    "print(f\"\\nSelected model: {winner_name} with params: {search_results[winner_name]['best_params']}\")\n",
    "\n",
    "winner.fit(X, y)\n",
    "train_pred = winner.predict(X)\n",
    "print(\"\\nConfusion matrix (train fit):\\n\", confusion_matrix(y, train_pred))\n",
    "print(\"\\nClassification report (train fit):\\n\", classification_report(y, train_pred, digits=3))\n",
    "\n",
    "# --- 6) Predict test set\n",
    "test_pred = winner.predict(X_test)\n",
    "test_pred_proba = winner.predict_proba(X_test) if hasattr(winner, \"predict_proba\") else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4027dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pump-it-up",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
